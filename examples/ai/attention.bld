# Self-Attention - Core transformer mechanism
# Attention(Q,K,V) = softmax(Q @ K^T / sqrt(d_k)) @ V
#
# BLD Analysis:
#   D: seq_len, embed_dim, num_heads (parallel dimensions)
#   B: masking (causal vs full), normalization
#   L: Q/K/V projections with learned rho, attention scores
#
# Key insight: attention IS structural alignment
# The softmax over scores = finding which keys align with each query
# High attention score = high alignment = information flows

structure attention

# Dimensions
D queries: seq_len_times_embed [parallel, input]
D keys: seq_len_times_embed [parallel, input]
D values: seq_len_times_embed [parallel, input]
D output: seq_len_times_embed [parallel, output]

# Projection dimensions (learned)
D W_q: embed_times_head_dim [parallel]
D W_k: embed_times_head_dim [parallel]
D W_v: embed_times_head_dim [parallel]
D W_o: head_dim_times_embed [parallel]

P num_heads: int
P head_dim: int
P scale: double

# Boundaries
B masking: causal | full | custom
  discriminator: mask_type
  causal -> lower_triangular(scores)
  full -> no_mask
  custom -> apply_mask(mask)

B normalize: pre_norm | post_norm | none
  discriminator: norm_position

# Links: the attention computation
# Q, K, V projections - parallel over heads
L project_q: queries -> Q (deps=0, rho=learnable)
L project_k: keys -> K (deps=0, rho=learnable)
L project_v: values -> V (deps=0, rho=learnable)

# Attention scores: Q @ K^T / sqrt(d_k)
# deps=0 because each query-key pair is independent
L compute_scores: Q -> scores (deps=0)

# Apply mask (boundary operation)
L apply_mask: scores -> masked_scores (deps=0)

# Softmax over keys (sequential within each query)
# deps=1 because softmax requires sum over all keys
L softmax: masked_scores -> attention_weights (deps=1)

# Weighted sum of values
# deps=0 because each output position is independent
L aggregate: attention_weights -> attended (deps=0)

# Output projection
L project_out: attended -> output (deps=0, rho=learnable)

returns: output
