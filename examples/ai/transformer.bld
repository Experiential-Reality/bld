# Transformer Block - Core LLM architecture
# Matches llama.cpp / GGUF model structure
#
# This describes the STRUCTURE of inference
# Weights come from GGUF file (rho values already learned)

structure transformer_block

# Dimensions
D hidden: hidden_dim [parallel]
D seq: seq_len [parallel]
D heads: n_heads [parallel]
D head_dim: head_dim [parallel]
D ffn: ffn_dim [parallel]

# Architecture parameters
P hidden_dim: int
P n_heads: int
P head_dim: int
P ffn_dim: int
P n_layers: int
P vocab_size: int
P rope_theta: double

# Boundaries
B norm_type: rmsnorm | layernorm
  discriminator: model_type

B attention_type: causal | bidirectional
  discriminator: is_causal
  causal -> apply_causal_mask

B quantization: f32 | f16 | q8_0 | q4_k | q5_k | q6_k
  discriminator: weight_type

# Links: the transformer computation
# Pre-attention normalization
L input_norm: hidden -> normed (deps=0)

# Q, K, V projections (weights from GGUF, rho already learned)
L project_q: normed -> Q (deps=0, rho=from_gguf)
L project_k: normed -> K (deps=0, rho=from_gguf)
L project_v: normed -> V (deps=0, rho=from_gguf)

# RoPE positional encoding
L rope: Q -> Q_rotated (deps=0)
L rope_k: K -> K_rotated (deps=0)

# Attention scores: Q @ K^T / sqrt(head_dim)
L attention_scores: Q_rotated -> scores (deps=0)
L attention_mask: scores -> masked (deps=0)
L attention_softmax: masked -> weights (deps=1)
L attention_output: weights -> attended (deps=0)

# Output projection
L project_out: attended -> attn_out (deps=0, rho=from_gguf)

# Residual connection
L residual_1: hidden + attn_out -> post_attn (deps=0)

# FFN
L ffn_norm: post_attn -> ffn_normed (deps=0)
L ffn_gate: ffn_normed -> gate (deps=0, rho=from_gguf)
L ffn_up: ffn_normed -> up (deps=0, rho=from_gguf)
L ffn_silu: gate -> gate_activated (deps=0)
L ffn_mul: gate_activated * up -> ffn_hidden (deps=0)
L ffn_down: ffn_hidden -> ffn_out (deps=0, rho=from_gguf)

# Residual connection
L residual_2: post_attn + ffn_out -> output (deps=0)

returns: output


# Full transformer stack
structure transformer

D layers: n_layers [sequential]
D tokens: seq_len [parallel]

P n_layers: int
P vocab_size: int

# Embedding (input)
L embed: token_ids -> hidden (deps=0, rho=from_gguf)

# Transformer blocks (sequential pipeline)
L blocks: hidden -> final_hidden (uses=transformer_block, deps=1)

# Output normalization
L final_norm: final_hidden -> normed (deps=0)

# LM head (vocabulary projection)
L lm_head: normed -> logits (deps=0, rho=from_gguf)

returns: logits
