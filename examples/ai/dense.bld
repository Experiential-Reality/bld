# Dense Layer - Fully connected neural network layer
# y = activation(W @ x + b)
#
# BLD Analysis:
#   D: input_dim, output_dim (parallel dimensions)
#   B: activation function (partitions output space)
#   L: weights with rho values (L-cost = coupling strength)
#
# Key insight: weights ARE correlations (rho values)
# L-cost formula: L = -0.5 * ln(1 - rho^2)
# High rho = strong coupling = important weight

structure dense

# Dimensions
D input: in_dim [parallel, input]
D output: out_dim [parallel, output]
D weights: in_dim_times_out_dim [parallel]
D bias: out_dim [parallel]

# Boundary: activation function partitions output space
# Each activation is a different mode of operation
B activation: linear | relu | sigmoid | tanh | softmax
  discriminator: activation_type
  linear -> passthrough
  relu -> max(0, x)
  sigmoid -> 1/(1+exp(-x))
  tanh -> tanh(x)
  softmax -> exp(x)/sum(exp(x))

# Links: the forward computation
# rho=learnable means these are trained parameters
L matmul: input -> pre_activation (deps=0, rho=learnable)
L add_bias: pre_activation -> pre_activation (deps=0, rho=learnable)
L activate: pre_activation -> output (deps=0)

returns: output
