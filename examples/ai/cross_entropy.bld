# Cross-Entropy Loss - Classification loss function
# loss = -sum(y_true * log(y_pred))
#
# BLD Analysis:
#   This IS alignment measurement
#   Low loss = high alignment between prediction and truth
#   The L-cost formula L = -0.5 * ln(1 - rho^2) is RELATED to cross-entropy
#   Both measure divergence between distributions

structure cross_entropy

D predictions: batch_times_classes [parallel, input]
D targets: batch_times_classes [parallel, input]
D loss: 1 [scalar, output]

# Boundary: handle edge cases
B edge_case: normal | zero_pred | perfect_pred
  discriminator: pred_range
  normal -> standard_computation
  zero_pred -> clip_to_epsilon
  perfect_pred -> zero_loss

# Links
# Step 1: Log of predictions (parallel, with clipping)
L log_pred: predictions -> log_preds (deps=0)

# Step 2: Element-wise multiply with targets (parallel)
L multiply: log_preds -> weighted (deps=0)

# Step 3: Sum over classes (reduction per sample)
L sum_classes: weighted -> sample_losses (deps=1)

# Step 4: Mean over batch (reduction)
L mean_batch: sample_losses -> loss (deps=1, hierarchy_depth=log2_batch)

returns: loss
